{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1980182-acaf-416e-b0ed-2414e26cb115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/fsboedij/Data/MouseAtlas')\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "import scanpy as sc\n",
    "import copy\n",
    "import torch\n",
    "import scarches as sca\n",
    "from scarches.dataset.trvae.data_handling import remove_sparsity\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import gdown\n",
    "import sankey\n",
    "import anndata\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from pySankey.sankey import sankey\n",
    "from matplotlib.pyplot import rc_context\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize']=(8,8) #rescale figures\n",
    "sc.settings.verbosity = 3\n",
    "sc.settings.set_figure_params(dpi=200, frameon=False)\n",
    "sc.set_figure_params(dpi=200)\n",
    "sc.set_figure_params(figsize=(5, 5))\n",
    "torch.set_printoptions(precision=10, sci_mode=False, edgeitems=12)\n",
    "pd.options.mode.chained_assignment = None\n",
    "import matplotlib.colors as mcolors\n",
    "import utils\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.cm as cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2573563e-109a-437f-8054-bb08db3ed20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata=sc.read('Data/Integration/Step10_fulladata_final_0411.h5ad')\n",
    "print(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73b1942-7d80-4cda-87de-84d6c0bb3ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "adata = adata[adata.obs['healthy_or_disease'] == 'Healthy']\n",
    "\n",
    "#Taking out the 'Post-Sendai' set due to the large epi composition\n",
    "\n",
    "adata = adata[adata.obs['study'] != 'Mouse_Post_Sendai']\n",
    "#full_latent = sc.read(\"/data/kkovacs/Python/Data/Kyle_Atlas_emb.h5ad\")\n",
    "\n",
    "#full_latent.obs_names_make_unique()\n",
    "\n",
    "#full_latent = full_latent[adata.obs_names,:]\n",
    "#adata.obsm['X_MLCA'] = full_latent.X.copy()\n",
    "\n",
    "adata.X.data=adata.layers['raw'].data.copy()\n",
    "\n",
    "dir_output='/home/fsboedij/Data/MouseAtlas/Data/others/'\n",
    "dir_results=dir_output\n",
    "\n",
    "FIGURES = dict()\n",
    "\n",
    "viral_to_cont = {\"Viral\": 1, \"Post-Viral\": 0.5, \"Non-Viral\": 0}\n",
    "adata.obs[\"viral_status_num\"] = adata.obs.Viral.map(viral_to_cont).astype(float)\n",
    "\n",
    "sex_to_cont = {\"Male\": 1, \"Female\": 0}\n",
    "adata.obs[\"sex_cont\"] = adata.obs.sex.map(sex_to_cont).astype('category')\n",
    "\n",
    "adata.obs['age_cont'] = adata.obs['age']\n",
    "adata.obs['age'] = adata.obs['age'].astype(object)\n",
    "#adata.obs['age_cont'].loc[adata.obs['age_cont'].isin(['1','5-6','6-8','8','10-12'])] = 'Young'\n",
    "#adata.obs['age_cont'].loc[adata.obs['age_cont'].isin(['12','6-20','12-13','16-20','13','14'])] = 'Adult'\n",
    "#adata.obs['age_cont'].loc[adata.obs['age_cont'].isin(['18-20','78','104'])] = 'Old'\n",
    "adata.obs['age'].loc[adata.obs['age'].isin(['5-6'])] = '5.5'\n",
    "adata.obs['age'].loc[adata.obs['age'].isin(['6-8'])] = '7'\n",
    "adata.obs['age'].loc[adata.obs['age'].isin(['10-12'])] = '11'\n",
    "adata.obs['age'].loc[adata.obs['age'].isin(['6-20'])] = '13'\n",
    "adata.obs['age'].loc[adata.obs['age'].isin(['12-13'])] = '12.5'\n",
    "adata.obs['age'].loc[adata.obs['age'].isin(['16-20'])] = '18'\n",
    "adata.obs['age'].loc[adata.obs['age'].isin(['18-20'])] = '19'\n",
    "adata.obs['age'] = adata.obs['age'].astype(float)\n",
    "print(adata.obs['age'].value_counts())\n",
    "\n",
    "\n",
    "adata.obs['pct_counts_mt']=adata.obs['percent.mt'].copy()\n",
    "adata.obs['pct_counts_mt'] = adata.obs['pct_counts_mt'].astype(float)\n",
    "\n",
    "adata.obs['total_counts'] = adata.obs['total_counts'].astype(float)\n",
    "\n",
    "adata.obs['Level_3_5']=adata.obs['Level_3'].copy()\n",
    "adata.obs['Level_3_5']=adata.obs['Level_3_5'].astype(object)\n",
    "adata.obs['Level_3_5'].loc[(adata.obs['Level_3'].isin([ 'T cell lineage']))] = adata.obs.Level_4\n",
    "adata.obs['Level_3_5'].loc[(adata.obs['Level_4'].isin([ 'Mast cells']))] = adata.obs.Level_4\n",
    "adata.obs['Level_3_5'].loc[(adata.obs['Level_4'].isin([ 'Club_Goblet']))] = adata.obs.Level_4\n",
    "adata.obs['Level_3_5'].loc[(adata.obs['Level_4'].isin([ 'Smoke induced club_goblet']))] = 'Club_Goblet'\n",
    "adata.obs['Level_3_5'].loc[(adata.obs['Level_5'].isin([ 'Lymphatic endothelial']))] = adata.obs.Level_5\n",
    "#adata.obs['Level_3_5'].loc[(adata.obs['Level_3'].isin([ 'Dendritic cells']))] = adata.obs.Level_4\n",
    "adata.obs['Level_3_5'].loc[(adata.obs['Level_5'].isin([ 'pDC']))] = adata.obs.Level_5\n",
    "#adata.obs['Level_3_5'].loc[(adata.obs['Level_4'].isin([ 'Neutrophils']))] = adata.obs.Level_5\n",
    "#adata.obs['Level_3_5'].loc[(adata.obs['Level_3_5'].isin([ 'Proliferating neutrophils']))] ='Neutrophils'\n",
    "\n",
    "#adata.obs['Level_3_5'].loc[(adata.obs['Level_3'].isin([ 'Capillary endothelial','Venous endothelial']))] = 'Endothelial cells'\n",
    "\n",
    "\n",
    "\n",
    "# Display the value counts of the new 'age_group' variable\n",
    "print(adata.obs['age_cont'].value_counts())\n",
    "def contains_zero_width_space(s):\n",
    "    return '\\u200b' in str(s)\n",
    "\n",
    "# Iterate through observation variables and check for \\u200b\n",
    "variables_with_zero_width_space = []\n",
    "\n",
    "for variable in adata.obs.columns:\n",
    "    if adata.obs[variable].dtype.kind == 'O':  # Check if the variable is of object (string) type\n",
    "        # Apply function only to non-null string values\n",
    "        if any(adata.obs[variable].dropna().apply(lambda x: isinstance(x, str) and contains_zero_width_space(x))):\n",
    "            variables_with_zero_width_space.append(variable)\n",
    "\n",
    "\n",
    "# Display variables with \\u200b\n",
    "if variables_with_zero_width_space:\n",
    "    print(\"Observation variables with \\\\u200b:\")\n",
    "    for variable in variables_with_zero_width_space:\n",
    "        print(f\"- {variable}\")\n",
    "else:\n",
    "    print(\"No observation variables contain \\\\u200b.\")\n",
    "\n",
    "\n",
    "\n",
    "'study','Disease',\n",
    "\n",
    "covariates = ['platform','viral_status_num',\n",
    "\t'sex_cont','age','Level_3_5','strain','total_counts',\n",
    "\t'pct_counts_mt']\n",
    "\n",
    "int_types = [\"integrated\"]  # list needs to include \"integrated\" and/or \"unintegrated\"\n",
    "\n",
    "if \"unintegrated\" in int_types:\n",
    "    n_pcs = 30\n",
    "\n",
    "min_n_cells_total = 50  # in total\n",
    "min_n_cells = 10  # per sample\n",
    "min_n_samples = 2\n",
    "\n",
    "verbose = True\n",
    "\n",
    "\n",
    "# Initiate a dictionary, in which we will store which samples were included per\n",
    "# single regression\n",
    "samples_included = dict()\n",
    "cts_to_skip = list()  # we will fill this with cts that have too few samples\n",
    "# these samples should be skipped in the rest of the analysis.\n",
    "# Loop through specified integration types:\n",
    "for int_type in int_types:\n",
    "    samples_included[int_type] = dict()\n",
    "    # loop through all annotations (= cell types), and also include the entire\n",
    "    # atlas as a \"cell type\":\n",
    "    for subset in sorted(adata.obs.Level_3_5.unique()) + [\"whole_atlas\"]:\n",
    "        # remove space from cell type name, for file storing etc.\n",
    "        subset_no_space = subset.replace(\" \", \"_\")\n",
    "        # initiate a dataframe, in which we will store for every covariate\n",
    "        # which samples we include in the regression (we will exclude samples\n",
    "        # that have no value available for the covariate under consideration\n",
    "        # (e.g. BMI not recorded), as well as samples that have fewer than 10\n",
    "        # cells of the cell type under consideration)\n",
    "        samples_included[int_type][subset] = pd.DataFrame(\n",
    "            index=adata.obs[\"sample\"].unique(), columns=covariates\n",
    "        )\n",
    "        # if file doesn't exist yet (built in this check for if loop breaks halfway,\n",
    "        # feel free to take it out):\n",
    "        if not os.path.isfile(\n",
    "            os.path.join(\n",
    "                dir_results,\n",
    "                f\"variance_explained_fractions/variance_explained_fractions_{subset_no_space}_{int_type}.csv\",\n",
    "            )\n",
    "        ):\n",
    "            print(f\"Working on {int_type}, {subset}...\")\n",
    "            # select the correct cells:\n",
    "            if subset == \"whole_atlas\":\n",
    "                subadata = adata.copy()\n",
    "                verbose = True\n",
    "            elif subset not in adata.obs.Level_3_5.unique():\n",
    "                raise ValueError(\n",
    "                    \"subset should be set either to 'Whole atlas' or to a category in your manual_ann grouped obs variable!\"\n",
    "                )\n",
    "            else:\n",
    "                subadata = adata[adata.obs.Level_3_5 == subset, :].copy()\n",
    "                verbose = False\n",
    "            if subadata.n_obs < min_n_cells_total:\n",
    "                print(f\"{subset} has fewer than {min_n_cells_total} cells! Skipping.\")\n",
    "                continue\n",
    "            # select the right embedding:\n",
    "            if int_type == \"unintegrated\":\n",
    "                emb_name = \"X_pca\"\n",
    "                sc.tl.pca(subadata, n_comps=n_pcs, use_highly_variable=True)\n",
    "            elif int_type == \"integrated\":\n",
    "                emb_name = \"X_MLCA\"\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"emb_name should be set either to 'integrated' or 'unintegrated'\"\n",
    "                )\n",
    "            # store the number of components in our embedding of choice\n",
    "            n_comps = subadata.obsm[emb_name].shape[1]\n",
    "            # initiate a dataframe in which we'll store the variance explained\n",
    "            # by each covariate, plus the total variance (\"overall\") observed\n",
    "            var_explained = pd.DataFrame(\n",
    "                index=range(n_comps), columns=covariates + [\"overall\"]\n",
    "            )\n",
    "            # initiate a dataframe in which we will store the data\n",
    "            # for our linear regression (i.e. the PC/latent components, + covariates).\n",
    "            # Rows are cells, but we will collapse this to samples below\n",
    "            comp_sample_df = pd.DataFrame(index=subadata.obs.index)\n",
    "            comp_sample_df[\"sample\"] = subadata.obs[\"sample\"]\n",
    "            # prepare aggregation dictionary for collapsing into sample-wise\n",
    "            # observations\n",
    "            agg_dict = {\"sample\": \"count\"}  # this will be the number of cells\n",
    "            for comp in range(n_comps):\n",
    "                # store component scores per cell\n",
    "                comp_sample_df[f\"comp{comp}\"] = subadata.obsm[emb_name][:, comp]\n",
    "                # we will aggregate these later by taking the mean per sample\n",
    "                agg_dict[f\"comp{comp}\"] = \"mean\"\n",
    "            for cov in covariates:\n",
    "                if cov in [\"log10_total_counts\", \"mito_frac\"]:\n",
    "                    # store values\n",
    "                    comp_sample_df[cov] = subadata.obs[cov]\n",
    "                    # we will aggregate by taking the mean\n",
    "                    agg_dict[cov] = \"mean\"\n",
    "                else:\n",
    "                    # for all other covariates: these are sample-level\n",
    "                    # covariates, so we will take the \"first\" observation\n",
    "                    # in the sample (which should be the only)\n",
    "                    comp_sample_df[cov] = subadata.obs[cov]\n",
    "                    agg_dict[cov] = \"first\"\n",
    "            # now collapse into sample-level observations\n",
    "            sample_df = (\n",
    "                comp_sample_df.groupby(\"sample\")\n",
    "                .agg(agg_dict)\n",
    "                .rename(columns={\"sample\": \"n_cells\"})\n",
    "            )\n",
    "            print()\n",
    "            # filter out samples with fewer than min_n_cells cells of the cell type\n",
    "            for cov in covariates:\n",
    "                if pd.api.types.is_categorical_dtype(sample_df[cov]):\n",
    "                    sample_df[cov] = sample_df[cov].astype(\"category\")\n",
    "            sample_df = sample_df.loc[\n",
    "                sample_df.n_cells >= min_n_cells,\n",
    "            ].copy()\n",
    "            # check number of samples left.\n",
    "            # If fewer than min_n_samples remain, we will skip the cell type\n",
    "            if sample_df.shape[0] < min_n_samples:\n",
    "                print(\n",
    "                    f\"Only {sample_df.shape[0]} samples available for {subset}. Skipping.\"\n",
    "                )\n",
    "                cts_to_skip.append(subset)\n",
    "                continue\n",
    "            # Otherwise, move on to the linear regression:\n",
    "            # do a linear regression on each component, with the component scores\n",
    "            # as response variable...\n",
    "            for comp in range(n_comps):\n",
    "                # store the component values (for all samples i.e. unfiltered)\n",
    "                y_true_unfiltered = sample_df.loc[:, f\"comp{comp}\"].values\n",
    "                # and store variance of y_true as \"overall\" variance\n",
    "                var_explained.loc[f\"comp{comp}\", \"overall\"] = np.var(y_true_unfiltered)\n",
    "                # and the covariate as fixed variable\n",
    "                for cov in covariates:\n",
    "                    # store covariate observations under x\n",
    "                    x = sample_df[cov].values.copy()\n",
    "                    # store samples to which they match\n",
    "                    x_samples = sample_df.index\n",
    "                    # check which of these samples have no observation (e.g.\n",
    "                    # because BMI was unknown, or age, etc.)\n",
    "                    # (the function used below checks for different kinds of\n",
    "                    # nas, e.g. np.nan, \"nan\", None, \"None\" etc.)\n",
    "                    x_nans = np.vectorize(pd.isna)(x)\n",
    "                    # now keep only xs that have real observations\n",
    "                    x = x[~x_nans]\n",
    "                    # if only one or no observations are left, skip this covariate\n",
    "                    if len(x) < 2:\n",
    "                        continue\n",
    "                    # filter samples according to x filtering\n",
    "                    x_samples = x_samples[~x_nans]\n",
    "                    # and store which samples were included in our samples_included\n",
    "                    # dictionary, for later reference (this is our \"n\")\n",
    "                    samples_included[int_type][subset][cov] = samples_included[\n",
    "                        int_type\n",
    "                    ][subset].index.isin(x_samples.tolist())\n",
    "                    # filter y_true according to x's filtering\n",
    "                    y_true = y_true_unfiltered[~x_nans].reshape(-1, 1)\n",
    "                    # prepare x for linear regression:\n",
    "                    # if it is a float (e.g. BMI, age), all we need to do is reshape:\n",
    "                    if x.dtype in [\"float32\", \"float\", \"float64\"]:\n",
    "                        x = x.reshape(-1, 1)\n",
    "                        # print that we are treating as numerical (only for first comp,\n",
    "                        # so that we don't print the same thing many times)\n",
    "                        if comp == 0 and verbose:\n",
    "                            print(f\"treating {cov} as continuous variable\")\n",
    "                    # otherwise we are dealing with a categorical...\n",
    "                    else:\n",
    "                        # if it has only one category, there is 0 variance and\n",
    "                        # we cannot perform linear regression. In that case,\n",
    "                        # move on to the next covariate.\n",
    "                        if len(set(x)) == 1:\n",
    "                            var_explained.loc[comp, cov] = np.nan\n",
    "                            continue\n",
    "                        # Otherwise, convert x to dummied variable:\n",
    "                        # print that we are converting to dummy\n",
    "                        # (only do it for the first comp, otherwise we print the same thing\n",
    "                        # many times)\n",
    "                        if comp == 0 and verbose:\n",
    "                            print(f\"converting {cov} to dummy variable\")\n",
    "                        # drop_first means we ensure that are x is full rank,\n",
    "                        # and we only encode all-1 categories\n",
    "                        x = pd.get_dummies(x, drop_first=True)\n",
    "                    # now perform linear regression\n",
    "                    lrf = LinearRegression(fit_intercept=True).fit(\n",
    "                        x,\n",
    "                        y_true,\n",
    "                    )\n",
    "                    # predict y based on the fit linear model\n",
    "                    y_pred = lrf.predict(x)\n",
    "                    # and store the variance of the predicted y, this is the\n",
    "                    # \"variance explained\" by the covariate, for this component\n",
    "                    var_explained.loc[comp, cov] = np.var(y_pred)\n",
    "            # for each covariate, sum up how much variance it explains across\n",
    "            # the components (i.e. PCs or scANVI latent components)\n",
    "            # Sort covariates from explaining most to explaining least\n",
    "            total_variance_explained = np.sum(var_explained, axis=0).sort_values(\n",
    "                ascending=False\n",
    "            )\n",
    "            # divide this by the total variance that was observed in the\n",
    "            # components, to get fraction of variance explained\n",
    "            total_variance_explained_fractions = (\n",
    "                total_variance_explained / total_variance_explained[\"overall\"]\n",
    "            )\n",
    "            # write to files:\n",
    "            # 1) variance explained fractions, for this integration type and cell type\n",
    "            total_variance_explained_fractions.to_csv(\n",
    "                os.path.join(\n",
    "                    dir_results,\n",
    "                    f\"variance_explained_fractions/variance_explained_fractions_{subset_no_space}_{int_type}.csv\",\n",
    "                )\n",
    "            )\n",
    "            # 2) samples included, for this cell type\n",
    "            samples_included[int_type][subset].to_csv(\n",
    "                os.path.join(\n",
    "                    dir_results,\n",
    "                    f\"samples_included/samples_included_{subset_no_space}.csv\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "int_type = \"integrated\" # we used integrated in the paper, not unintegrated\n",
    "\n",
    "subsets = [\"whole_atlas\"] + sorted(adata.obs.Level_3_5.unique())\n",
    "# import variance explained fractions for the Whole atlas, to get indices\n",
    "# and columns for dataframes\n",
    "vef_df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        dir_results,\n",
    "        f\"variance_explained_fractions/variance_explained_fractions_whole_atlas_{int_type}.csv\",\n",
    "    ),\n",
    "    index_col=0,\n",
    ").rename(columns={\"0\": \"uncorrected\"})\n",
    "\n",
    "# inititate empty dataframes for variance explained fractions\n",
    "vars_expl = pd.DataFrame(\n",
    "    index=[subs for subs in subsets if subs not in cts_to_skip], columns=vef_df.index\n",
    ")\n",
    "# initiate an empty dictionary to store samples included for each cell type - cov\n",
    "# pair\n",
    "samples_included = dict()\n",
    "# loop through cell types + Whole atlas\n",
    "for subset in subsets:\n",
    "    if subset not in cts_to_skip:\n",
    "        # replace spaces in cell type name with \"_\" for file writing etc.\n",
    "        subset_no_space = subset.replace(\" \", \"_\")\n",
    "        # import variance explained fractions:\n",
    "        vef_df = pd.read_csv(\n",
    "            os.path.join(\n",
    "                dir_results,\n",
    "                f\"variance_explained_fractions/variance_explained_fractions_{subset_no_space}_{int_type}.csv\",\n",
    "            ),\n",
    "            index_col=0,\n",
    "        ).rename(columns={\"0\": \"uncorrected\"})\n",
    "        # store values in our overview dataframes\n",
    "        \n",
    "        vars_expl.loc[subset, vef_df.index] = vef_df.values.reshape(1, -1)\n",
    "        # store which samples were included for each covariate based on matching file\n",
    "        samples_included[subset] = pd.read_csv(\n",
    "            os.path.join(\n",
    "                dir_results, f\"samples_included/samples_included_{subset_no_space}.csv\"\n",
    "            ),\n",
    "            index_col=0,\n",
    "        )\n",
    "\n",
    "\n",
    "# rename \"whole_atlas\" to \"Whole atlas\" index, for prettier plotting\n",
    "vars_expl.rename(index={\"whole_atlas\": \"Whole atlas\"}, inplace=True)\n",
    "\n",
    "\n",
    "cat_covs = [\n",
    "    cat\n",
    "    for cat in covariates\n",
    "    if adata.obs[cat].values.dtype\n",
    "    not in [\n",
    "        \"float32\",\n",
    "        \"float\",\n",
    "        \"float64\",\n",
    "    ]\n",
    "]\n",
    "# and continuous/numerical covariates\n",
    "cont_covs = [\n",
    "    cat\n",
    "    for cat in covariates\n",
    "    if adata.obs[cat].values.dtype\n",
    "    in [\n",
    "        \"float32\",\n",
    "        \"float\",\n",
    "        \"float64\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# aggregate values by sample\n",
    "agg_dict = {cov: \"first\" for cov in cat_covs}  # use \"first\" for categorical\n",
    "agg_dict.update({cov: \"mean\" for cov in cont_covs})  # and \"mean\" for continuous\n",
    "# collect statistics:\n",
    "sample_df = adata.obs.groupby(\"sample\").agg(agg_dict)\n",
    "# initiate dataframe where we store for every cell type, for every covariate,\n",
    "# how many unique values are observed\n",
    "n_cats_per_cov = pd.DataFrame(\n",
    "    index=[subs for subs in subsets if subs not in cts_to_skip],\n",
    "    columns=cat_covs + cont_covs + [\"sample\"],\n",
    ")\n",
    "\n",
    "# loop through cell types and categorical covariates\n",
    "for subset in subsets:\n",
    "    if subset not in cts_to_skip:\n",
    "        for cov in cat_covs:\n",
    "            # annotation will always be one (this might change if we set our\n",
    "            # cell type variable differently!)\n",
    "            if cov == \"Level_3_5\":\n",
    "                n_cats_per_cov[cov] = 1\n",
    "            else:\n",
    "                # extract covariate values only for samples included for cell type\n",
    "                # and covariate\n",
    "                sample_df_subset = sample_df.loc[\n",
    "                    samples_included[subset].index[samples_included[subset][cov]], :\n",
    "                ].copy()\n",
    "                # count number of unique values\n",
    "                n_per_cov = sample_df_subset[cov].nunique()\n",
    "                # store\n",
    "                n_cats_per_cov.loc[subset, cov] = n_per_cov\n",
    "        # store number of samples included as the maximum number of samples included,\n",
    "        # across covariates, for a cell type\n",
    "        n_cats_per_cov.loc[subset, \"sample\"] = (\n",
    "            samples_included[subset].sum(axis=0).max()\n",
    "        )\n",
    "\n",
    "# now create boolean df indicating in which cases only one unique value\n",
    "# was observed for a cell type covariate pair\n",
    "nis1 = n_cats_per_cov == 1\n",
    "nis1.index = list(nis1.index)\n",
    "nis1.loc[\"Whole atlas\", :] = [\n",
    "    adata.obs[cat_cov].nunique() == 1 for cat_cov in nis1.columns\n",
    "]\n",
    "#  now loop through continuous covariates and calculate variance instead of number\n",
    "# of unique variables. If variance is 0, set nis1 to True.\n",
    "for subset in subsets:\n",
    "    if subset not in cts_to_skip:\n",
    "        for cov in cont_covs:\n",
    "            sample_df_subset = sample_df.loc[\n",
    "                samples_included[subset].index[\n",
    "                    (samples_included[subset].sum(axis=1) > 0)\n",
    "                ],\n",
    "                :,\n",
    "            ].copy()\n",
    "            variance = np.var(sample_df_subset[cov])\n",
    "            if variance == 0 or np.isnan(variance):\n",
    "                nis1.loc[subset, cov] = True\n",
    "            else:\n",
    "                nis1.loc[subset, cov] = False\n",
    "\n",
    "\n",
    "\n",
    "nis1[\"overall\"] = False\n",
    "\n",
    "\n",
    "n_samples_included_df = pd.DataFrame(index=samples_included.keys(), columns=covariates)\n",
    "for ct, samp_incl in samples_included.items():\n",
    "    n_samples_included_df.loc[ct, :] = samp_incl.loc[:, covariates].sum(axis=0)\n",
    "\n",
    "cts_ordered = [\n",
    "    ct\n",
    "    for ct in n_cats_per_cov.sort_values(by=\"sample\", ascending=False).index.tolist()\n",
    "    if ct != \"whole_atlas\"\n",
    "]\n",
    "\n",
    "technical_covariates = [\n",
    "    \"study\",\n",
    "    \"platform\",\n",
    "    \"pct_counts_mt\",\n",
    "    \"Level_5\",\n",
    "    \"total_counts\"\n",
    "]\n",
    "\n",
    "# specify bio covariates to show\n",
    "bio_covariates = [\n",
    "\t\"viral_status_num\",\n",
    "    \"sex_cont\",\n",
    "    \"age\",\n",
    "\t\"Disease\",\n",
    "    \"strain\"\n",
    "]\n",
    "\n",
    "vars_expl.sort_values(by=\"Whole atlas\", axis=1, ascending=False, inplace=True)\n",
    "technical_covariates = [cov for cov in vars_expl.columns if cov in technical_covariates]\n",
    "bio_covariates = [cov for cov in vars_expl.columns if cov in bio_covariates]\n",
    "# now re-order vars_expl:\n",
    "vars_expl = vars_expl.loc[\n",
    "    [\"Whole atlas\"] + cts_ordered,\n",
    "    technical_covariates + bio_covariates,\n",
    "]\n",
    "\n",
    "vars_expl_to_plot = vars_expl.copy()\n",
    "# add capitalization\n",
    "col_remapper = {\n",
    "    col: (\" \".join(col.replace(\"_\", \" \").split(\" \"))).capitalize()\n",
    "    for col in vars_expl.columns\n",
    "}\n",
    "# remove \"short\" suffices\n",
    "\n",
    "col_remapper[\"Level_3_5\"] = \"Cell type\"\n",
    "# We re-name some of the categories for clarity in the paper:\n",
    "renaming = {\n",
    "    \"Group\": \"Mice conditions\",\n",
    "    \"subject_type\": \"Subject status\",\n",
    "    \"pct_counts_mt\": \"Fraction of mitochondrial UMIs\",\n",
    "    \"viral_status_num\": \"Viral status\",\n",
    "}\n",
    "col_remapper.update(renaming)\n",
    "# now rename:\n",
    "#vars_expl_to_plot.rename(columns=col_remapper, inplace=True)\n",
    "# and remove dataset from figure\n",
    "#vars_expl_to_plot.drop(columns=\"s\", inplace=True)\n",
    "\n",
    "mask1 = (\n",
    "    n_samples_included_df.loc[\n",
    "        [\"whole_atlas\"] + vars_expl.index[1:].tolist(),\n",
    "        vars_expl.columns,\n",
    "    ]\n",
    "    < 5\n",
    ")\n",
    "\n",
    "# store mask for cell type - covariate pairs for which only 1 unique value\n",
    "# was observed\n",
    "mask2 = nis1.loc[\n",
    "    [\"whole_atlas\"] + vars_expl.index[1:].tolist(),\n",
    "    vars_expl.columns,\n",
    "]\n",
    "\n",
    "# combine masks\n",
    "mask_overall = mask1 | mask2\n",
    "# rename Whole atlas\n",
    "mask_overall.rename(index={\"whole_atlas\": \"Whole atlas\"}, inplace=True)\n",
    "\n",
    "#n_non_masked_rows = mask_overall.shape[0] - (mask_overall == True).sum(axis=0)\n",
    "#columns_to_drop = n_non_masked_rows.loc[n_non_masked_rows < 5].index.tolist()\n",
    "\n",
    "columns_to_drop=['viral_status_num']\n",
    "#index_to_drop = ['Viral induced Gran','Viral induced monocytes']\n",
    "vars_expl_to_plot.drop(columns=columns_to_drop, inplace=True)\n",
    "#vars_expl_to_plot.drop(index=index_to_drop, inplace=True)\n",
    "\n",
    "#mask_overall.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "fz = 13  # fontsize\n",
    "cmap = copy.copy(cm.get_cmap(\"Reds\"))  # select color map\n",
    "cmap.set_bad(\"grey\")  # set masked color to grey instead of white\n",
    "fig, ax = plt.subplots(figsize=(11, 11))  # figsize\n",
    "vmax = 0.4\n",
    "title = \"Fraction of total variance\"\n",
    "# for file storing:\n",
    "title_no_spaces = title.replace(\" \", \"_\")\n",
    "sns.heatmap(\n",
    "    vars_expl_to_plot.astype(float),\n",
    "    ax=ax,\n",
    "    vmax=vmax,\n",
    "    cbar_kws={\"extend\": \"max\"},\n",
    "    mask=mask_overall.loc[vars_expl_to_plot.index, vars_expl_to_plot.columns].values,\n",
    "    cmap=cmap,\n",
    ")\n",
    "ax.collections[0].colorbar.set_label(f\"{title} explained\", fontsize=fz + 4)\n",
    "ax.set_xticklabels(ax.get_xmajorticklabels(),rotation=45, fontsize=fz+3)\n",
    "ax.set_yticklabels(ax.get_ymajorticklabels(), fontsize=fz+3)\n",
    "plt.title(\n",
    "    f\"{title} explained per covariate in integrated embedding\\n(30 scANVI latent dimensions)\",\n",
    "    fontsize=fz+3,\n",
    ")\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=fz + 4)\n",
    "plt.xlabel(\"Covariate (technical)             Covariate (biological)\", fontsize=fz+3)\n",
    "plt.ylabel(\"Cell type\", fontsize=fz+3)\n",
    "plt.tight_layout()\n",
    "plt.grid(False)\n",
    "plt.savefig('Control_Covariate_technical.png')\n",
    "plt.show()\n",
    "FIGURES[f\"4a_{title_no_spaces}_explained_per_covariate_{int_type}_emb\"] = fig\n",
    "\n",
    "\n",
    "#find out what the variance is about, is the variance about cell composition \n",
    "#or cell something?\n",
    "\n",
    "for subset in subsets:\n",
    "    if subset not in cts_to_skip:\n",
    "        cols = [col for col in cat_covs if col not in [\"subject_ID\", \"Level_3_5\"]]\n",
    "        if subset not in \"whole_atlas\":\n",
    "            plt.clf()\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.scatter(n_cats_per_cov.loc[subset, cols], vars_expl.loc[subset, cols])\n",
    "            plt.xlabel(\"number of categories in cov\")\n",
    "            plt.ylabel(\"variance explained by cov\")\n",
    "            plt.title(subset)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"Variance_explained_by_covariance_{subset}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
